{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "# Natural Language Toolkit (NLTK)\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## 1. NLTK Corpora\n",
    "\n",
    "</font>\n",
    "\n",
    "[Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import nltk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Download necessary corpus "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "nltk.download('gutenberg')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Review content of loaded corpus "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.1 Gutenberg corpus\n",
    "\n",
    "</font>\n",
    "\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from nltk.corpus import gutenberg\r\n",
    "gutenberg.fileids()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Choose target file id and review raw text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "moby_dick_id= gutenberg.fileids()[-6]\r\n",
    "print ('moby_dick_id=', moby_dick_id)\r\n",
    "type(gutenberg.raw(moby_dick_id))\r\n",
    "print(gutenberg.raw(moby_dick_id)[:400])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "moby_dick_id= melville-moby_dick.txt\n",
      "[Moby Dick by Herman Melville 1851]\n",
      "\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\n",
      "\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\n",
      "known nations of the world.  He loved to dust his old grammars; it\n",
      "so\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Review tokenized text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "md_text= gutenberg.words(moby_dick_id)\r\n",
    "print ('Words number = {:,}'.format(len(md_text))) \r\n",
    "print ('Unique words number = {:,}\\n'.format(len(set(md_text)))) \r\n",
    "print (md_text[:30])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Words number = 260,819\n",
      "Unique words number = 19,317\n",
      "\n",
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Review sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "md_sent= gutenberg.sents(moby_dick_id)\r\n",
    "len(md_sent)\r\n",
    "md_sent[:3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'],\n",
       " ['ETYMOLOGY', '.'],\n",
       " ['(',\n",
       "  'Supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'Late',\n",
       "  'Consumptive',\n",
       "  'Usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'Grammar',\n",
       "  'School',\n",
       "  ')']]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print (list (set(md_text))[:20]) # Note : set requires converting  to list to apply slizing \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['exposure', 'uninhabited', 'ON', 'proof', 'cork', 'leisurely', 'Siberia', 'promotion', 'Tistig', 'crunching', 'JOIST', 'soft', 'embarks', 'hogs', 'alpine', 'bearskin', 'Bremen', 'opened', 'VISIT', 'commands']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Most frequent words\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Frequency distribution \r\n",
    "from nltk import FreqDist\r\n",
    "dist = FreqDist(md_text) # the same as text1.vocab() \r\n",
    "print (len(dist) , type (dist))\r\n",
    "dist"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19317 <class 'nltk.probability.FreqDist'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FreqDist({',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024, 'a': 4569, 'to': 4542, ';': 4072, 'in': 3916, 'that': 2982, ...})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "vocab = dist.keys()\r\n",
    "print (len(vocab))\r\n",
    "list (vocab)[:10]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19317\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Review frequent but not short words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "freq_words = [w for w in vocab if len(w) > 5 and dist[w] > 100] \r\n",
    "print (len(freq_words), freq_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "27 ['called', 'through', 'almost', 'whales', 'thought', 'before', 'against', 'towards', 'things', 'nothing', 'without', 'should', 'little', 'seemed', 'though', 'captain', 'himself', 'moment', 'CHAPTER', 'something', 'Captain', 'between', 'whaling', 'another', 'Queequeg', 'Pequod', 'Starbuck']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.2. Access the NLTK corpora\n",
    "\n",
    "   \n",
    "</font>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Determining the nltk_data location\n",
    "\n",
    "</font>\n",
    "\n",
    "import nltk\n",
    "<br>\n",
    "print (nltk.\\_\\_file\\_\\_) \n",
    "<br>\n",
    "in that directory in the `data.py` you may see the path to your nltk_data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "fp = nltk.__file__\r\n",
    "fp = os.path.join ('/'.join(fp.split('\\\\')[:-1]), 'data.py')\r\n",
    "print (fp)\r\n",
    "\r\n",
    "with open (fp , 'r') as f: \r\n",
    "    content = f.read()\r\n",
    "print (re.findall(r'NLTK data package might reside(?:.|\\s){,260}', content)[0])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:/VScode-projects/havrylov_ds/venv/lib/site-packages/nltk\\data.py\n",
      "NLTK data package might reside.\n",
      "   These directories will be checked in order when looking for a\n",
      "   resource in the data package.  Note that this allows users to\n",
      "   substitute in their own versions of resources, if they have them\n",
      "   (e.g., in their home directory under ~/nltk_data).\"\"\"\n",
      "\n",
      "# \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review NLTK corpora\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "target_directory= '/Users/otsebriy/nltk_data/corpora'\r\n",
    "print ( [x[0].replace(target_directory, '') for x in os.walk(target_directory)])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '/stopwords', '/brown', '/words', '/inaugural', '/gutenberg', '/nonbreaking_prefixes', '/wordnet', '/state_union', '/genesis']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.4. Words corpus \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "nltk.download('words')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from nltk.corpus import words\r\n",
    "correct_spellings = words.words()\r\n",
    "print (correct_spellings[:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac', 'abaca', 'abacate', 'abacay', 'abacinate', 'abacination', 'abaciscus', 'abacist', 'aback', 'abactinal', 'abactinally', 'abaction', 'abactor', 'abaculus', 'abacus', 'Abadite', 'abaff', 'abaft', 'abaisance', 'abaiser', 'abaissed', 'abalienate', 'abalienation', 'abalone', 'Abama', 'abampere', 'abandon', 'abandonable', 'abandoned', 'abandonedly', 'abandonee', 'abandoner', 'abandonment', 'Abanic', 'Abantes', 'abaptiston', 'Abarambo', 'Abaris', 'abarthrosis', 'abarticular', 'abarticulation', 'abas', 'abase', 'abased', 'abasedly', 'abasedness', 'abasement', 'abaser', 'Abasgi', 'abash', 'abashed', 'abashedly', 'abashedness', 'abashless', 'abashlessly', 'abashment', 'abasia', 'abasic', 'abask', 'Abassin', 'abastardize', 'abatable', 'abate', 'abatement', 'abater', 'abatis', 'abatised', 'abaton', 'abator', 'abattoir', 'Abatua', 'abature', 'abave', 'abaxial', 'abaxile', 'abaze', 'abb', 'Abba', 'abbacomes', 'abbacy', 'Abbadide']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## 2. Simple NLP Tasks \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.1. Tokenize\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Custom Tokenizer\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import re\r\n",
    "# This was used for parcing keywords for tokens \r\n",
    "def tokenize_sentence(target_str):\r\n",
    "    target_str = re.sub(r\"[_-]\", ' ', target_str) # to get single tokens and apply the n-grams later \r\n",
    "    re_pattern = r\"[\\w\\']+\" \r\n",
    "    tokens = re.findall(re_pattern, target_str)\r\n",
    "    return [token for token in tokens if len(token) > 2] \r\n",
    "tokenize_sentence(\"I like Ternopil. I don't care of Ivano-Frankivs'k\" )"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['like', 'Ternopil', \"don't\", 'care', 'Ivano', \"Frankivs'k\"]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### NLTK sentence tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "sent_tokenize()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from nltk.tokenize import sent_tokenize\r\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? '''\r\n",
    "print (target_text)\r\n",
    "sent_tokenize(target_text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hello Mr. Smith!',\n",
       " 'How are you doing today?',\n",
       " 'The whether is great and the Python is awesome.',\n",
       " 'The sky is blue.',\n",
       " 'The pen costs $2.45.',\n",
       " 'What about pensil?',\n",
       " 'Is ... the same price?']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### NLTK word tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "word_tokenize()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'''\r\n",
    "print (target_text)\r\n",
    "print (word_tokenize(target_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.\n",
      "['Hello', 'Mr.', 'Smith', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'The', 'whether', 'is', 'great', 'and', 'the', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'blue', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.2. Stop words\n",
    "\n",
    "</font>\n",
    "\n",
    "stopwords()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "stop_words= set(stopwords.words('english'))\r\n",
    "# stop_words= set(stopwords.words('russian'))\r\n",
    "print ('Total len : {}'.format(len(list(stop_words))))\r\n",
    "print(list(stop_words)[:50])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total len : 179\n",
      "['your', 'who', 'more', 'don', 'in', 'now', 'on', 'do', \"aren't\", 'by', 'won', \"don't\", 'where', 've', 'shouldn', 'me', 'it', 'weren', 't', 'she', 'then', 'up', \"isn't\", 'too', 'mustn', 'under', \"you've\", \"should've\", 'wasn', \"weren't\", 'so', 'hadn', 'these', 'until', 'because', \"hadn't\", 'being', 'ain', 'through', 'from', 'him', 'he', 'they', 'out', 'some', 'has', 'can', \"couldn't\", 'while', 'doing']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'\r\n",
    "tokens = word_tokenize(target_text)\r\n",
    "filtered_tokens= [t for t in tokens if t not in stop_words]\r\n",
    "\r\n",
    "print (filtered_tokens)\r\n",
    "print ('\\nExcluded tokens (stopwords): \\n{}'.format([t for t in tokens if t in stop_words]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'example', 'showing', 'stop', 'words', 'filtering', '.', 'Hello', 'Mr.', 'Smith', '!', 'How', 'today', '?', 'The', 'whether', 'great', 'Python', 'awesome', '.', 'The', 'sky', 'blue', '.']\n",
      "\n",
      "Excluded tokens (stopwords): \n",
      "['is', 'an', 'the', 'are', 'you', 'doing', 'is', 'and', 'the', 'is', 'is']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Stemming \n",
    "\n",
    "</font>\n",
    "\n",
    "PorterStemmer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import  word_tokenize\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue '\r\n",
    "\r\n",
    "words= np.array(word_tokenize(target_text))\r\n",
    "ps= PorterStemmer()\r\n",
    "v_stem= np.vectorize(ps.stem)\r\n",
    "stemmed_words= v_stem(words)\r\n",
    "\r\n",
    "print (target_text)\r\n",
    "stemmed_words\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['thi', 'is', 'an', 'exampl', 'show', 'the', 'stop', 'word',\n",
       "       'filter', '.', 'hello', 'mr.', 'smith', '!', 'how', 'are', 'you',\n",
       "       'do', 'today', '?', 'the', 'whether', 'is', 'great', 'and', 'the',\n",
       "       'python', 'is', 'awesom', '.', 'the', 'sky', 'is', 'blue'],\n",
       "      dtype='<U7')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# one more sample\r\n",
    "target_text = 'go, went, gone, going, gonna, goes, goings'\r\n",
    "words= np.array(word_tokenize(target_text))\r\n",
    "v_stem(words)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['go', ',', 'went', ',', 'gone', ',', 'go', ',', 'gon', 'na', ',',\n",
       "       'goe', ',', 'go'], dtype='<U4')"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# another sample ) \r\n",
    "target_text = 'apply, applied, applies, applying , applyings'\r\n",
    "words= np.array(word_tokenize(target_text))\r\n",
    "v_stem(words)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['appli', ',', 'appli', ',', 'appli', ',', 'appli', ',', 'appli'],\n",
       "      dtype='<U5')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Lemmatizing\n",
    "\n",
    "</font>\n",
    "\n",
    "WordNetLemmatizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Without specified POS  \n",
    "\n",
    "</font>\n",
    "noun by default "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from nltk.corpus import wordnet as wn\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "wn_lemmatizer= WordNetLemmatizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "[wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] # v - means verb"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['apply', ',', 'applied', ',', 'applies', ',', 'applying', ',', 'applyings']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# sampl w/o pos tag \r\n",
    "target_text = 'is, am, are, was, were, been, being'\r\n",
    "print([wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['is', ',', 'am', ',', 'are', ',', 'wa', ',', 'were', ',', 'been', ',', 'being']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to verb "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print (target_text)\r\n",
    "[wn_lemmatizer.lemmatize(w,'v') for w in word_tokenize(target_text)] # v - means verb # the same as the following:\r\n",
    "# [wn_lemmatizer.lemmatize(w,wn.VERB) for w in word_tokenize(target_text)] "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is, am, are, was, were, been, being\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to noun "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "target_text = 'car, cars'\r\n",
    "[wn_lemmatizer.lemmatize(w,wn.NOUN) for w in word_tokenize(target_text)] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['car', ',', 'car']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.4. Part of Speech (POS)\n",
    "\n",
    "</font>\n",
    "\n",
    "pos_tag()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "nltk.download('state_union')\r\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\r\n",
    "from nltk.corpus import state_union\r\n",
    "train_text= state_union.raw('2005-GWBush.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print (train_text[:500])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "sentences = sent_tokenize(train_text)\r\n",
    "s = sentences [10]\r\n",
    "s"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Over the next several months, on issue after issue, let us do what Americans have always done, and build a better world for our children and our grandchildren.'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "words= word_tokenize(s)\r\n",
    "print ('words:\\n{}'.format(words))\r\n",
    "tagged= nltk.pos_tag(words)\r\n",
    "print ('\\ntags:\\n{}'.format (tagged))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "words:\n",
      "['Over', 'the', 'next', 'several', 'months', ',', 'on', 'issue', 'after', 'issue', ',', 'let', 'us', 'do', 'what', 'Americans', 'have', 'always', 'done', ',', 'and', 'build', 'a', 'better', 'world', 'for', 'our', 'children', 'and', 'our', 'grandchildren', '.']\n",
      "\n",
      "tags:\n",
      "[('Over', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('on', 'IN'), ('issue', 'NN'), ('after', 'IN'), ('issue', 'NN'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('do', 'VB'), ('what', 'WP'), ('Americans', 'NNPS'), ('have', 'VBP'), ('always', 'RB'), ('done', 'VBN'), (',', ','), ('and', 'CC'), ('build', 'VB'), ('a', 'DT'), ('better', 'JJR'), ('world', 'NN'), ('for', 'IN'), ('our', 'PRP$'), ('children', 'NNS'), ('and', 'CC'), ('our', 'PRP$'), ('grandchildren', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review NLTK POS\n",
    "\n",
    "</font>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "nltk.download('tagsets')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "nltk.help.upenn_tagset()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Ambiguity of NLTK POS tagging \n",
    "\n",
    "</font>\n",
    "NLTK tags the only one the most probable of possible cases\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "s = 'visiting friends can be surprising'\r\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(s)) # alternative possible case [('visiting', 'JJ'),...] # friends is the subject\r\n",
    "tagged # visiting is the subject"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('visiting', 'VBG'),\n",
       " ('friends', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('surprising', 'JJ')]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Converting POS formats \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def penn_to_wn(treebank_tag):\r\n",
    "    if treebank_tag.startswith('J'):\r\n",
    "        return wn.ADJ\r\n",
    "    elif treebank_tag.startswith('V'):\r\n",
    "        return wn.VERB\r\n",
    "    elif treebank_tag.startswith('N'):\r\n",
    "        return wn.NOUN\r\n",
    "    elif treebank_tag.startswith('R'):\r\n",
    "        return wn.ADV\r\n",
    "    else:\r\n",
    "        return ''\r\n",
    "\r\n",
    "# def penn_to_wn(tag):\r\n",
    "#     return get_wordnet_pos(tag)\r\n",
    "\r\n",
    "\r\n",
    "{t[0]: penn_to_wn(t[1]) for t in tagged }\r\n",
    "\r\n",
    "# Part-of-speech constants\r\n",
    "#  ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'visiting': 'v', 'friends': 'n', 'can': '', 'be': 'v', 'surprising': 'a'}"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Mention about \r\n",
    "# 1) pos used in recognizing location \r\n",
    "# 2) pos used in language model to predict placeholders"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Chunks of POS\n",
    "\n",
    "</font>\n",
    "\n",
    "RegexpParser()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import nltk \r\n",
    "from nltk.corpus import state_union\r\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
    "train_text= state_union.raw('2005-GWBush.txt')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Cyberoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "\r\n",
    "sentences = sent_tokenize(train_text)\r\n",
    "\r\n",
    "for s in sentences[13:15]:\r\n",
    "    print('='*40, '\\nsentence= {}'.format(s))\r\n",
    "    words= word_tokenize(s)\r\n",
    "    # print ('words= {}'.format(words))\r\n",
    "    tagged= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\r\n",
    "    print('\\ntagged:\\n{}'.format(tagged))\r\n",
    "\r\n",
    "    # detect chunks \r\n",
    "    chunk_gram = 'chunk: {<JJ.?>+<NN.?>+} ' # selects adj and noun followed by\r\n",
    "    \r\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\r\n",
    "    chunked = chunk_parser.parse(tagged)    \r\n",
    "    print('\\nchunked:\\n{}'.format(chunked.flatten))\r\n",
    "\r\n",
    "    # extract chunks \r\n",
    "    chunks = []\r\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'chunk'):\r\n",
    "        chunk = \"\"\r\n",
    "        for leaf in subtree.leaves():\r\n",
    "            chunk += leaf[0] + ' '\r\n",
    "        chunks.append(chunk.strip())\r\n",
    "\r\n",
    "    print('\\nDetected chunks:\\n{}'.format(chunks))\r\n",
    "    \r\n",
    "#     draw tree \r\n",
    "#     chunked.draw()\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================== \n",
      "sentence= America's economy is the fastest growing of any major industrialized nation.\n",
      "\n",
      "tagged:\n",
      "[('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('fastest', 'JJS'), ('growing', 'NN'), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), Tree('chunk', [('fastest', 'JJS'), ('growing', 'NN')]), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['fastest growing']\n",
      "======================================== \n",
      "sentence= In the past four years, we provided tax relief to every person who pays income taxes, overcome a recession, opened up new markets abroad, prosecuted corporate criminals, raised homeownership to its highest level in history, and in the last year alone, the United States has added 2.3 million new jobs.\n",
      "\n",
      "tagged:\n",
      "[('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), ('new', 'JJ'), ('markets', 'NNS'), ('abroad', 'RB'), (',', ','), ('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS'), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), ('highest', 'JJS'), ('level', 'NN'), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('year', 'NN'), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), ('new', 'JJ'), ('jobs', 'NNS'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), Tree('chunk', [('new', 'JJ'), ('markets', 'NNS')]), ('abroad', 'RB'), (',', ','), Tree('chunk', [('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS')]), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), Tree('chunk', [('highest', 'JJS'), ('level', 'NN')]), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), Tree('chunk', [('last', 'JJ'), ('year', 'NN')]), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), Tree('chunk', [('new', 'JJ'), ('jobs', 'NNS')]), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['new markets', 'prosecuted corporate criminals', 'highest level', 'last year', 'new jobs']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample of visualization \n",
    "due to issue with `chunked.draw()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "\r\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\r\n",
    "print ('Tagged:\\n', sentence)\r\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \r\n",
    "\r\n",
    "cp = nltk.RegexpParser(grammar) \r\n",
    "result = cp.parse(sentence)\r\n",
    "print(result)\r\n",
    "# result.draw()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tagged:\n",
      " [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = \"chunks.png\" height=500 width= 500 align=\"left\">\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from nltk.tree import Tree\r\n",
    "s = 'Donald came to New York for summit'\r\n",
    "words= word_tokenize(s)\r\n",
    "tagged_words= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\r\n",
    "chunked= nltk.ne_chunk(tagged_words)\r\n",
    "# print ('named_entities:\\n{}'.format(chunked))\r\n",
    "\r\n",
    "def extract_ne_chunks(chunked): \r\n",
    "    chunks = []\r\n",
    "    for i in chunked:\r\n",
    "        if type(i) == Tree:    \r\n",
    "            print (i.flatten)\r\n",
    "            chunks.append(\" \".join([token for token, pos in i.leaves()]))\r\n",
    "    return chunks\r\n",
    "\r\n",
    "\r\n",
    "extract_ne_chunks(chunked)\r\n",
    "\r\n",
    "# chunked.draw()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bound method Tree.flatten of Tree('PERSON', [('Donald', 'NNP')])>\n",
      "<bound method Tree.flatten of Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Donald', 'New York']"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.5. Wordnet\n",
    "\n",
    "</font>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from nltk.corpus import wordnet as wn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synsets\n",
    "\n",
    "</font>\n",
    "\n",
    "wn.synsets()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "target_word = 'look'\r\n",
    "print('target_word = {} '.format(target_word))\r\n",
    "\r\n",
    "synsets = wn.synsets(target_word)\r\n",
    "print (synsets[0])\r\n",
    "[synset.name() for  synset  in synsets]\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_word = look \n",
      "Synset('expression.n.01')\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['expression.n.01',\n",
       " 'look.n.02',\n",
       " 'look.n.03',\n",
       " 'spirit.n.02',\n",
       " 'look.v.01',\n",
       " 'look.v.02',\n",
       " 'look.v.03',\n",
       " 'search.v.02',\n",
       " 'front.v.01',\n",
       " 'attend.v.02',\n",
       " 'look.v.07',\n",
       " 'expect.v.03',\n",
       " 'look.v.09',\n",
       " 'count.v.08']"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Lemmas\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.lemmas()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "for synset in synsets:\r\n",
    "    print ('{}:{}'.format(synset.name(), [lemma.name() for  lemma in synset.lemmas()]))\r\n",
    "\r\n",
    "print ('\\n',synsets[0].lemmas()[2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expression.n.01:['expression', 'look', 'aspect', 'facial_expression', 'face']\n",
      "look.n.02:['look', 'looking', 'looking_at']\n",
      "look.n.03:['look']\n",
      "spirit.n.02:['spirit', 'tone', 'feel', 'feeling', 'flavor', 'flavour', 'look', 'smell']\n",
      "look.v.01:['look']\n",
      "look.v.02:['look', 'appear', 'seem']\n",
      "look.v.03:['look']\n",
      "search.v.02:['search', 'look']\n",
      "front.v.01:['front', 'look', 'face']\n",
      "attend.v.02:['attend', 'take_care', 'look', 'see']\n",
      "look.v.07:['look']\n",
      "expect.v.03:['expect', 'look', 'await', 'wait']\n",
      "look.v.09:['look']\n",
      "count.v.08:['count', 'bet', 'depend', 'look', 'calculate', 'reckon']\n",
      "\n",
      " Lemma('expression.n.01.aspect')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.definition()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "for synset in synsets:\r\n",
    "    print ('{}:\\t{}'.format(synset.name(), synset.definition()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expression.n.01:\tthe feelings expressed on a person's face\n",
      "look.n.02:\tthe act of directing the eyes toward something and perceiving it visually\n",
      "look.n.03:\tphysical appearance\n",
      "spirit.n.02:\tthe general atmosphere of a place or situation and the effect that it has on people\n",
      "look.v.01:\tperceive with attention; direct one's gaze towards\n",
      "look.v.02:\tgive a certain impression or have a certain outward aspect\n",
      "look.v.03:\thave a certain outward or facial expression\n",
      "search.v.02:\tsearch or seek\n",
      "front.v.01:\tbe oriented in a certain direction, often with respect to another reference point; be opposite to\n",
      "attend.v.02:\ttake charge of or deal with\n",
      "look.v.07:\tconvey by one's expression\n",
      "expect.v.03:\tlook forward to the probable occurrence of\n",
      "look.v.09:\taccord in appearance with\n",
      "count.v.08:\thave faith or confidence in\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Examples\n",
    "\n",
    "</font>\n",
    "synset.examples()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "for synset in synsets:\r\n",
    "    print ('{}:\\t{}\\n'.format(synset.name(), synset.examples()))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expression.n.01:\t['a sad expression', 'a look of triumph', 'an angry face']\n",
      "\n",
      "look.n.02:\t['he went out to have a look', 'his look was fixed on her eyes', 'he gave it a good looking at', 'his camera does his looking for him']\n",
      "\n",
      "look.n.03:\t[\"I don't like the looks of this place\"]\n",
      "\n",
      "spirit.n.02:\t['the feel of the city excited him', 'a clergyman improved the tone of the meeting', 'it had the smell of treason']\n",
      "\n",
      "look.v.01:\t['She looked over the expanse of land', 'Look at your child!', 'Look--a deer in the backyard!']\n",
      "\n",
      "look.v.02:\t['She seems to be sleeping', 'This appears to be a very difficult problem', 'This project looks fishy', 'They appeared like people who had not eaten or slept for a long time']\n",
      "\n",
      "look.v.03:\t['How does she look?', 'The child looks unhappy', 'She looked pale after the surgery']\n",
      "\n",
      "search.v.02:\t['We looked all day and finally found the child in the forest', 'Look elsewhere for the perfect gift!']\n",
      "\n",
      "front.v.01:\t['The house looks north', 'My backyard look onto the pond', 'The building faces the park']\n",
      "\n",
      "attend.v.02:\t['Could you see about lunch?', 'I must attend to this matter', 'She took care of this business']\n",
      "\n",
      "look.v.07:\t['She looked her devotion to me']\n",
      "\n",
      "expect.v.03:\t['We were expecting a visit from our relatives', 'She is looking to a promotion', 'he is waiting to be drafted']\n",
      "\n",
      "look.v.09:\t[\"You don't look your age!\"]\n",
      "\n",
      "count.v.08:\t['you can count on me to help you any time', 'Look to your friends for support', 'You can bet on that!', 'Depend on your family in times of crisis']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synonyms and Antonyms\n",
    "\n",
    "</font>\n",
    "synset.lemmas()\n",
    "<br>lemma.antonyms()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def get_syn_ant(target_word):\r\n",
    "    synonyms = []\r\n",
    "    antonyms = []\r\n",
    "    print('target_word = {} '.format(target_word))\r\n",
    "\r\n",
    "    for syn in wn.synsets(target_word):\r\n",
    "        for l in syn.lemmas(): # use lemmas to obtain synonyms\r\n",
    "            synonyms.append(l.name())\r\n",
    "            if l.antonyms():  # some lemmas have antonyms\r\n",
    "                antonyms.append({la.name(): l.name() for la in l.antonyms()})\r\n",
    "#                 antonyms.append(l.antonyms()[0].name())  # Assuming selecting just first antonym for lemma\r\n",
    "    return set(synonyms), antonyms\r\n",
    "\r\n",
    "target_word = 'good'\r\n",
    "synonyms, antonyms =  get_syn_ant (target_word)\r\n",
    "print ('\\nsynonyms:', synonyms),\r\n",
    "print ('antonyms:\\n', antonyms)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target_word = good \n",
      "\n",
      "synonyms: {'full', 'respectable', 'right', 'practiced', 'unspoiled', 'well', 'honest', 'just', 'dear', 'salutary', 'proficient', 'estimable', 'goodness', 'in_force', 'sound', 'undecomposed', 'safe', 'effective', 'unspoilt', 'ripe', 'serious', 'beneficial', 'skilful', 'secure', 'trade_good', 'thoroughly', 'near', 'good', 'soundly', 'adept', 'upright', 'expert', 'in_effect', 'skillful', 'commodity', 'dependable', 'honorable'}\n",
      "antonyms:\n",
      " [{'evil': 'good'}, {'evilness': 'goodness'}, {'bad': 'good'}, {'badness': 'goodness'}, {'bad': 'good'}, {'evil': 'good'}, {'ill': 'well'}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "words_pairs= [('drink.n.01', 'water.n.01'),\r\n",
    "              ('drink.n.01', 'milk.n.01'),\r\n",
    "              ('milk.n.01', 'water.n.01'),\r\n",
    "              ('drink.n.01', 'coca_cola.n.01'),\r\n",
    "              ('pepsi.n.01', 'coca_cola.n.01'),\r\n",
    "              ('New_year.n.01', 'Santa_Claus.n.01'),\r\n",
    "              ('New_year.n.01', 'christmas.n.01')\r\n",
    "              ]\r\n",
    "for words_pair in words_pairs:\r\n",
    "    w1 = wn.synset(words_pair[0])\r\n",
    "    w2 = wn.synset(words_pair[1])\r\n",
    "    print('{}  vs {}: {}'.format(w1.name(), w2.name(), w1.wup_similarity(w2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "drink.n.01  vs water.n.01: 0.26666666666666666\n",
      "drink.n.01  vs milk.n.01: 0.25\n",
      "milk.n.01  vs water.n.01: 0.42105263157894735\n",
      "drink.n.01  vs coca_cola.n.01: 0.2222222222222222\n",
      "pepsi.n.01  vs coca_cola.n.01: 0.9090909090909091\n",
      "new_year.n.01  vs santa_claus.n.01: 0.25\n",
      "new_year.n.01  vs christmas.n.01: 0.7142857142857143\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.6. Edit Distance\n",
    "\n",
    "</font>\n",
    "\n",
    "nltk.edit_distance()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "target_pairs= [\r\n",
    "    ('hello', 'hell'),\r\n",
    "    ('hell', 'hall'),\r\n",
    "    ('men', 'manual'),\r\n",
    "    ('муха', 'слон'),\r\n",
    "    ('casual', 'causal'),\r\n",
    "    ('top', 'pot'),\r\n",
    "    ('top', 'open')\r\n",
    "    \r\n",
    "]\r\n",
    "for word_1, word_2 in target_pairs:\r\n",
    "    print ('{}, {}: {}/{}'.format(word_1, word_2, nltk.edit_distance(word_1, word_2,transpositions=True), \r\n",
    "                                 nltk.edit_distance(word_1, word_2,transpositions=False)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hello, hell: 1/1\n",
      "hell, hall: 1/1\n",
      "men, manual: 4/4\n",
      "муха, слон: 4/4\n",
      "casual, causal: 1/2\n",
      "top, pot: 2/2\n",
      "top, open: 3/3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.7. Wordnet similarity\n",
    "\n",
    "</font>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wordnet hierarchy\n",
    "\n",
    "</font>\n",
    "WordNet organizes information in a hierarchy \n",
    "Verbs, nouns, adjectives etc. all have separate hierarchy \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "%%html\r\n",
    "<img src = 'wn_tree1.png' height=500 width= 500 align=\"left\">\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<img src = 'wn_tree1.png' height=500 width= 500 align=\"left\">\n"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Path similarity\n",
    "\n",
    "</font>\n",
    "\n",
    "$1/(n+1)$ , $n$ - steps between concepts\n",
    "<br>\n",
    "synset_1.path_similarity(synset_2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "deer_synset = wn.synset('deer.n.01')\r\n",
    "elk_synset=  wn.synset('elk.n.01')\r\n",
    "horse_synset=  wn.synset('horse.n.01')\r\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.path_similarity(elk_synset))) # d= 2 \r\n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.path_similarity(horse_synset))) # d= 7 "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deer and elk: 0.5\n",
      "deer and horse: 0.14285714285714285\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wu-Palmer similarity\n",
    "\n",
    "</font>\n",
    "synset_1.wup_similarity(synset_2)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# comparing to wup_similarity\r\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.wup_similarity(elk_synset)))  \r\n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.wup_similarity(horse_synset))) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deer and elk: 0.967741935483871\n",
      "deer and horse: 0.8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# import editdistance # standalone package\r\n",
    "\r\n",
    "def calc_words_similarity(word_a,word_b, verbose= 0):\r\n",
    "    ''' Calculates wup similarity for all synsets if synsets available, and score based on edit distance otherwise\r\n",
    "        in case of synsets available returns max of all values\r\n",
    "    '''\r\n",
    "    word_a_synsets= wn.synsets(word_a)\r\n",
    "    word_b_synsets= wn.synsets(word_b)\r\n",
    "    if len (word_a_synsets)==0 or len (word_b_synsets)==0:\r\n",
    "        if verbose: \r\n",
    "            print ('One of words has no synsets')\r\n",
    "        return calc_edit_score(word_a,word_b)\r\n",
    "    else:\r\n",
    "        similarities_all_synsets= []\r\n",
    "        for word_a_synset in word_a_synsets:\r\n",
    "            for word_b_synset in word_b_synsets:\r\n",
    "                if word_a_synset.pos() == word_b_synset.pos():  # this speeds up the calculation                   \r\n",
    "                    synsets_similarity =  word_a_synset.wup_similarity(word_b_synset)\r\n",
    "                    if synsets_similarity: # some time return None\r\n",
    "                        similarities_all_synsets.append(synsets_similarity)\r\n",
    "                    else: # synsets_similarity is None\r\n",
    "                        synsets_similarity= 0 \r\n",
    "                else: # different pos \r\n",
    "                    synsets_similarity= 0 \r\n",
    "                similarities_all_synsets.append(synsets_similarity)\r\n",
    "                if verbose:\r\n",
    "                    print (word_a_synset.name(), word_b_synset.name(), synsets_similarity)\r\n",
    "        return max(similarities_all_synsets)\r\n",
    "\r\n",
    "def calc_edit_score(w1, w2):\r\n",
    "    '''calc the score being based on edit (Levinshtein) distance\r\n",
    "        1 difference - 0.8, 2- 0.64, 3-.51 ...\r\n",
    "    '''\r\n",
    "    edit_dist= nltk.edit_distance(w1, w2)    \r\n",
    "    if edit_dist> len (w1) or edit_dist> len (w2): # no common letter\r\n",
    "        print ('No common letter. Consider similarity as 0.')\r\n",
    "        return 0\r\n",
    "    return 0.8 ** edit_dist\r\n",
    "\r\n",
    "calc_words_similarity('drink','pepsi', verbose= 1)\r\n",
    "# calc_words_similarity('deer','horse', verbose= 1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "drink.n.01 pepsi.n.01 0.2222222222222222\n",
      "drink.n.02 pepsi.n.01 0.18181818181818182\n",
      "beverage.n.01 pepsi.n.01 0.8421052631578947\n",
      "drink.n.04 pepsi.n.01 0.2857142857142857\n",
      "swallow.n.02 pepsi.n.01 0.25\n",
      "drink.v.01 pepsi.n.01 0\n",
      "drink.v.02 pepsi.n.01 0\n",
      "toast.v.02 pepsi.n.01 0\n",
      "drink_in.v.01 pepsi.n.01 0\n",
      "drink.v.05 pepsi.n.01 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Custom phrase similarity\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def calc_phrase_similarity(phrase_a, phrase_b):\r\n",
    "    '''\r\n",
    "    phrase_a, phrase_b - str\r\n",
    "    '''\r\n",
    "    if len (phrase_a) * len (phrase_b)== 0:\r\n",
    "        return 0\r\n",
    "\r\n",
    "    tokens_a = nltk.word_tokenize(phrase_a)\r\n",
    "    tokens_a = [w for w in tokens_a if w not in stop_words]\r\n",
    "\r\n",
    "    tokens_b = nltk.word_tokenize(phrase_b)\r\n",
    "    tokens_b = [w for w in tokens_b if w not in stop_words]\r\n",
    "\r\n",
    "    \r\n",
    "    a_similarity =  target_phrase_avg(tokens_a, tokens_b)\r\n",
    "    b_similarity =  target_phrase_avg(tokens_b, tokens_a)\r\n",
    "\r\n",
    "    return np.mean([a_similarity, b_similarity])\r\n",
    "\r\n",
    "def target_phrase_avg(tokens_a, tokens_b):\r\n",
    "    all_similarities= [target_word_similarity_score(tokens_a, b) for b in tokens_b]\r\n",
    "    if len(all_similarities)>0:\r\n",
    "        return np.mean(all_similarities)\r\n",
    "    else:\r\n",
    "        return 0\r\n",
    "\r\n",
    "\r\n",
    "def target_word_similarity_score(current_words, target_word):\r\n",
    "    '''\r\n",
    "    Calculates similarity between all current words (all synsets) and ONE target word (all synsets)\r\n",
    "    :param current_words: list of words of current phrase\r\n",
    "    :param target_word: str\r\n",
    "    :return: float\r\n",
    "    '''\r\n",
    "    if len(current_words)==0:\r\n",
    "        return 0\r\n",
    "    all_current_words_similarities= []\r\n",
    "    for current_word in current_words:\r\n",
    "        all_current_words_similarities.append(calc_words_similarity(current_word, target_word))\r\n",
    "    if len(all_current_words_similarities)==0:\r\n",
    "        return 0\r\n",
    "    return max(all_current_words_similarities)\r\n",
    "\r\n",
    "phrase_a = 'Data Science is fascinating'\r\n",
    "phrase_b = 'Work with data is interesting'\r\n",
    "\r\n",
    "# phrase_a = 'Data Science is fascinating'\r\n",
    "# phrase_b = 'the milk is white'\r\n",
    "\r\n",
    "print ('{}\\n{}'.format(phrase_a, phrase_b))\r\n",
    "calc_phrase_similarity(phrase_a, phrase_b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Science is fascinating\n",
      "Work with data is interesting\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8634920634920635"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "from nltk.corpus import gutenberg "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 1\n",
    "\n",
    "</font>\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in `moby_raw`?\n",
    "<br>*This function should return an integer.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def example_one():\r\n",
    "    from nltk.tokenize import word_tokenize\r\n",
    "    return len(word_tokenize(moby_raw)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "print ('{:,}'.format(example_one()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "255,028\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 2\n",
    "\n",
    "</font>\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does `moby_raw` have?\n",
    "<br>*This function should return an integer.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "def example_two():    \r\n",
    "    return len(set(nltk.word_tokenize(moby_raw)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "print ('{:,}'.format(example_two()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20,742\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 3\n",
    "\n",
    "</font>\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does `moby_raw` have?\n",
    "<br>*This function should return an integer.*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "\r\n",
    "def example_three():\r\n",
    "    lemmatizer = WordNetLemmatizer()\r\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in nltk.word_tokenize(moby_raw)]\r\n",
    "    return len(set(lemmatized))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "print ('{:,}'.format(example_three()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16,887\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 1\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "<br>*This function should return a float.*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def answer_one():\r\n",
    "    return example_two()/example_one()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "answer_one()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.08133224587104161"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.08139566804842562`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 2\n",
    "\n",
    "</font>\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "<br>*This function should return a float.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk import FreqDist\r\n",
    "def answer_two():\r\n",
    "    words_freq = FreqDist(word_tokenize(moby_raw))\r\n",
    "    whale_count = words_freq['Whale'] + words_freq['whale']\r\n",
    "    return whale_count/example_one()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "answer_two()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.004125037250811676"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.4125668166077752`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 3\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "<br>*This function should return a list of 10 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "def answer_three():\r\n",
    "    words_freq = FreqDist(word_tokenize(moby_raw))\r\n",
    "    sorted_list = [(k, v) for k, v in sorted(words_freq.items(), key=lambda item: item[1], reverse=True)]\r\n",
    "    return sorted_list[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "answer_three()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978)]"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(',', 19204),\n",
    " ('the', 13715),\n",
    " ('.', 7308),\n",
    " ('of', 6513),\n",
    " ('and', 6010),\n",
    " ('a', 4545),\n",
    " ('to', 4515),\n",
    " (';', 4173),\n",
    " ('in', 3908),\n",
    " ('that', 2978)]`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 4\n",
    "\n",
    "</font>\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "<br>*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "def answer_four():\r\n",
    "    vocab = list(set(nltk.word_tokenize(moby_raw)))\r\n",
    "    words_freq = FreqDist(word_tokenize(moby_raw))\r\n",
    "    result_words = [word for word in vocab if len(word) > 5 and words_freq[word] > 150] \r\n",
    "    return sorted(result_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "print (answer_four())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 5\n",
    "\n",
    "</font>\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "<br>\n",
    "*This function should return a tuple `(longest_word, length)`.*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "def answer_five():\r\n",
    "    vocab = list(set(nltk.word_tokenize(moby_raw)))\r\n",
    "    words_len = [(word, len(word)) for word in vocab]\r\n",
    "    return max(words_len ,key=lambda item:item[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "answer_five()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`(\"twelve-o'clock-at-night\", 23)`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 6\n",
    "\n",
    "</font>\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "from string import punctuation\r\n",
    "\r\n",
    "def answer_six():\r\n",
    "    words_freq = FreqDist(word_tokenize(moby_raw))\r\n",
    "    uniq_words_freq = [(v, k) for k, v in sorted(words_freq.items(), key=lambda item: item[1], reverse=True) if v > 2000 and k not in punctuation]\r\n",
    "    return uniq_words_freq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "print(answer_six())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2113, 'I')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2097, 'I')]`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 7\n",
    "\n",
    "</font>\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "<br>*This function should return a float.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\r\n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "def answer_seven():\r\n",
    "    average_token_num = [len(word_tokenize(sentence)) for sentence in sent_tokenize(moby_raw)]\r\n",
    "    return np.array(average_token_num).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "print(answer_seven())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25.88591149005278\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`25.881952902963864`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 8\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "from collections import Counter\r\n",
    "\r\n",
    "def answer_eight():\r\n",
    "    tags = nltk.pos_tag(word_tokenize(moby_raw))\r\n",
    "    counted_speech_parts = Counter([y for (x,y) in tags])\r\n",
    "    top_5_freq_parts = [(k, v) for k, v in sorted(counted_speech_parts.items(), key=lambda item: item[1], reverse=True)]\r\n",
    "    return top_5_freq_parts[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "print(answer_eight())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('NN', 32727), ('IN', 28662), ('DT', 25879), (',', 19204), ('JJ', 17613)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 9\n",
    "\n",
    "</font>\n",
    "\n",
    "Create spelling recommender, that take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest `edit distance` (you may need  to use `nltk.edit_distance(word_1, word_2, transpositions=True)`), and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Recommender should provide recommendations for the three words: `['cormulent', 'incendenece', 'validrate']`.\n",
    "<br>*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "from nltk.corpus import words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "def answer_nine(default_words= ['cormulent', 'incendenece', 'validrate']):\r\n",
    "    return None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "answer_nine()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['corpulent', 'intendence', 'validate']`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "NLTK 3.4 documentation\n",
    "<br>\n",
    "https://www.nltk.org/\n",
    "\n",
    "Accessing Text Corpora and Lexical Resources\n",
    "<br>\n",
    "https://www.nltk.org/book/ch02.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.chunk package\n",
    "<br>\n",
    "https://www.nltk.org/api/nltk.chunk.html\n",
    "\n",
    "Edit distance\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Edit_distance\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "7453dc2649e5d2e951287478b1f2cbd03ebfe097d0eed2c2c148fe698e2d62af"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}