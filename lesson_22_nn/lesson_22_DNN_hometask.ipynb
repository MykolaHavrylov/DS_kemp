{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "# Deep learning\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_1.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Notation\n",
    "\n",
    "</font>\n",
    "\n",
    "$n_{x}$ - number of features <br>\n",
    "$m$ - number of samples<br>\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n",
    "$L$ - number of layers (excluding input layer)<br> \n",
    "Index $[l]$ corresponds to layer number $l \\in (1...L)$ <br> \n",
    "Index $(i)$ corresponds to sample number $i \\in (1,m) $<br> \n",
    "Bottom index corresponds to unit number e.g. $a^{[2](3)}_{4}$ is the 4th activation unit in 2nd layer of 3rd sample<br> \n",
    "$n^{[l]}$  - number of units in layer $l$<br> \n",
    "$g^{[l]}$ - activation function of layer $l$ <br>\n",
    "$A^{[l]} = g^{[l]}(Z^{[l]})$  - pos-activation values of layer $l \\in (1...L)$  ($A^{[0]} = X$ ) <br>\n",
    "$W^{[l]}$ and $b^{[l]} $  - weights and bias of layer $l$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Shapes\n",
    "\n",
    "</font>\n",
    "\n",
    "#### Sample for 4 layer neural network \n",
    "\n",
    "\n",
    "<img src = \"data/19_2.png\" align = 'left'>\n",
    "<br>$\n",
    "\\quad n^{[0]} = n_{x} =  2\\\\\n",
    "\\quad n^{[1]} = 3\\\\\n",
    "\\quad n^{[2]} = 5\\\\\n",
    "\\quad n^{[3]} = 4\\\\\n",
    "\\quad n^{[4]} = 2\\\\\n",
    "\\quad n^{[5\n",
    "]} = 1$\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Shape of $\\quad W^{[1]} = (n^{[1]}, n^{[0]}) = (3,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[1]} = (n^{[1]}, 1) = (3,1)$ <br>\n",
    "Shape of $\\quad W^{[2]} = (n^{[2]}, n^{[1]})= (5,3)\\quad\\quad\\quad$     Shape of $\\quad b^{[2]} = (n^{[2]}, 1) = (5,1)$ <br>\n",
    "Shape of $\\quad W^{[3]} = (n^{[3]}, n^{[2]})= (4,5)\\quad\\quad\\quad$     Shape of $\\quad b^{[3]} = (n^{[3]}, 1) = (4,1)$ <br>\n",
    "Shape of $\\quad W^{[4]} = (n^{[4]}, n^{[3]})= (2,4)\\quad\\quad\\quad$     Shape of $\\quad b^{[4]} = (n^{[4]}, 1) = (2,1)$ <br>\n",
    "Shape of $\\quad W^{[5]} = (n^{[5]}, n^{[4]})= (1,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[5]} = (n^{[5]}, 1) = (1,1)$ <br>\n",
    "<br>In general:\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "<br>Shape of $$\\quad A^{[l]}, Z^{[l]}, \\frac{d\\mathcal{L}}{\\partial a^{[l]}}, \\frac{\\partial\\mathcal{L}}{\\partial z^{[l]}} = (n^{[l]}, m) \\quad,\\quad  \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = (n^{[l]}, 1)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Whole process: \n",
    "\\[LINEAR -> RELU\\] $\\times$ (L-1) -> \\[LINEAR -> SIGMOID\\]\n",
    "\n",
    "\n",
    "$A^{[0]} = X$ - input layer \n",
    "\n",
    "Iterate for all $l \\in (1.. L)$:\n",
    "\n",
    "$\\quad\\quad\\quad A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$$\n",
    "\n",
    "$\\hat{Y} = A^{[L]}$ - output layer (predicted value)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Cost Function\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation\n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n",
    "Iterate through all layres back:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\sigma\\,(z)=\\frac {1}{1+{e}^{-z}} \\Rightarrow \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "RELU(z) = max(0, z)\\Rightarrow \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward and backward propagation diagram\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_forward_backward.png\" align = 'left'>\n",
    "\n",
    "\n",
    "$$\\quad$$\n",
    "\n",
    "Note: It may be worth to cashe the $b^{[l]}$ parameters with other $ W^{[l]} ,A^{[l-1]}, Z^{[l]}$ to make sure the shape of $\\frac {\\partial\\mathcal{L}}{\\partial b^{[l]}}$ is the same as shape of $b^{[l]}$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Garadient descent \n",
    "\n",
    "</font>\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$$\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import h5py \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from scipy import ndimage"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Implementation steps \n",
    "\n",
    "</font>\n",
    "\n",
    " - Prepare data \n",
    " - Initialize parameters \n",
    " - Implement forward propagation step \n",
    " - Сompute сost\n",
    " - Init backward propagation\n",
    " - Implement backward propagation step\n",
    " - Update parameters in gradient descent \n",
    " - (Build two-layer model)\n",
    " - Train model \n",
    " - Evaluate model\n",
    " - Implement forward propagation whole process \n",
    " - Implement backward propagation whole process \n",
    " - Build deep neural network model \n",
    " - Predict \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "cwd= os.getcwd() # current working directory\r\n",
    "path = os.path.join(cwd,'data')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_dataset():\r\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\r\n",
    "    train_dataset = h5py.File(file_name, \"r\")\r\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\r\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\r\n",
    "\r\n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\r\n",
    "    test_dataset = h5py.File(file_name, \"r\")\r\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\r\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\r\n",
    "\r\n",
    "    classes = ['non-cat','cat']\r\n",
    "\r\n",
    "    return X_train, Y_train, X_test, Y_test, classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Review samples\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "index = 11\r\n",
    "# Implement the code to review the picture with index = 11 and print the labeled class \r\n",
    "plt.figure()\r\n",
    "plt.imshow(X_train[11])\r\n",
    "print(f\"y = {Y_train[11]}, it's a '{classes[Y_train[11]]}' picture\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y = 1, it's a 'cat' picture\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/cat_11.png\" >\n",
    "\n",
    "y = 1, it's a 'cat' picture."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Analyze dimensions   \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "m_train, num_px,_,_ = X_train.shape\r\n",
    "m_test =  X_test.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Implement the code to print the outpus provided in expected result\r\n",
    "print(f\"X_train.shape = {X_train.shape}\")\r\n",
    "print(f\"X_test.shape = {X_test.shape}\")\r\n",
    "\r\n",
    "print(f\"Y_train.shape = {Y_train.shape}\")\r\n",
    "print(f\"Y_test.shape = {Y_test.shape}\")\r\n",
    "\r\n",
    "print(f\"Number of training examples: m_train = {m_train}\")\r\n",
    "print(f\"Number of testing example: m_test = {m_test}\")\r\n",
    "\r\n",
    "print(f\"Height/Width of each image: num_px = {num_px}\")\r\n",
    "print(f\"Each image is of size: ({num_px}, {num_px}, 3)\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (209, 64, 64, 3)\n",
      "X_test.shape = (50, 64, 64, 3)\n",
      "Y_train.shape = (209,)\n",
      "Y_test.shape = (50,)\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing example: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\r\n",
    "\r\n",
    "###  Expected result\r\n",
    "\r\n",
    "</font>\r\n",
    "\r\n",
    "```\r\n",
    "X_train.shape=  (209, 64, 64, 3)\r\n",
    "X_test.shape=  (50, 64, 64, 3)\r\n",
    "Y_train.shape=  (209,)\r\n",
    "Y_test.shape=  (50,)\r\n",
    "Number of training examples: m_train = 209\r\n",
    "Number of testing examples: m_test = 50\r\n",
    "Height/Width of each image: num_px = 64\r\n",
    "Each image is of size: (64, 64, 3)\r\n",
    "```\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Reshape data\n",
    "\n",
    "</font>\n",
    "\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Reshape the data\r\n",
    "X_train_flatten = X_train.reshape(-1, m_train)\r\n",
    "X_test_flatten =  X_test.reshape(-1, m_test)\r\n",
    "\r\n",
    "Y_train = Y_train.reshape(-1, m_train)\r\n",
    "Y_test = Y_test.reshape(-1, m_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(f\"X_train_flatten shape: {X_train_flatten.shape}\")\r\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\r\n",
    "print(f\"X_test_flatten shape: {X_test_flatten.shape}\")\r\n",
    "print(f\"Y_test shape: {Y_test.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train_flatten shape: (12288, 209)\n",
      "Y_train shape: (1, 209)\n",
      "X_test_flatten shape: (12288, 50)\n",
      "Y_test shape: (1, 50)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\r\n",
    "\r\n",
    "###  Expected result\r\n",
    "\r\n",
    "</font>\r\n",
    "\r\n",
    "```\r\n",
    "X_train_flatten shape: (12288, 209)\r\n",
    "Y_train shape: (1, 209)\r\n",
    "X_test_flatten shape: (12288, 50)\r\n",
    "Y_test shape: (1, 50)\r\n",
    "```\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Scale data\n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_train_scaled = X_train_flatten/255.\r\n",
    "X_test_scaled = X_test_flatten/255."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Initialize parameters\n",
    "\n",
    "</font>\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "Note: Initialization of $W = random  \\,/   \\sqrt{ n^{[l-1]} }  $ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def initialize_parameters(layer_dims):\r\n",
    "    \"\"\"\r\n",
    "    layer_dims - list containing the dimensions of each layer in our network including input layer e.g. [12288,7,1]\r\n",
    "    Returns: dictionary with keys \"W\" and \"b\" and their values are dicts with keys corresponding to layers numbers.\r\n",
    "        for 'W' - value for every layer is weight matrix of shape (layer_dims[l], layer_dims[l-1])\r\n",
    "        for 'b' - bias vector of shape (layer_dims[l], 1)\r\n",
    "    \"\"\"    \r\n",
    "    np.random.seed(1)\r\n",
    "    parameters = {'W':{}, 'b':{}}\r\n",
    "\r\n",
    "    # Implement initialization using np.random.randn to match expected result\r\n",
    "    \r\n",
    "    parameters[\"W\"] = [np.random.randn(layer_dims[layer], layer_dims[layer - 1]) for layer in range(len(layer_dims))]\r\n",
    "    parameters[\"b\"] = [[np.zeros(l)] for l in layer_dims]\r\n",
    "    return parameters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# check the initialize_parameters()\r\n",
    "layer_dims= [2,3,5,1] \r\n",
    "params = initialize_parameters(layer_dims)\r\n",
    "for l in range(1,len(layer_dims)):\r\n",
    "    print ('W[{0}] =\\n{1}\\nb[{0}] =\\n{2}\\n'.format(l, params['W'][l], params['b'][l] ))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W[1] =\n",
      "[[-0.52817175 -1.07296862]\n",
      " [ 0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069 ]]\n",
      "b[1] =\n",
      "[array([0., 0., 0.])]\n",
      "\n",
      "W[2] =\n",
      "[[ 0.3190391  -0.24937038  1.46210794]\n",
      " [-2.06014071 -0.3224172  -0.38405435]\n",
      " [ 1.13376944 -1.09989127 -0.17242821]\n",
      " [-0.87785842  0.04221375  0.58281521]\n",
      " [-1.10061918  1.14472371  0.90159072]]\n",
      "b[2] =\n",
      "[array([0., 0., 0., 0., 0.])]\n",
      "\n",
      "W[3] =\n",
      "[[ 0.50249434  0.90085595 -0.68372786 -0.12289023 -0.93576943]]\n",
      "b[3] =\n",
      "[array([0.])]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\r\n",
    "\r\n",
    "###  Expected result\r\n",
    "\r\n",
    "</font>\r\n",
    "\r\n",
    "```\r\n",
    "W[1] =\r\n",
    "[[ 1.14858562 -0.43257711]\r\n",
    " [-0.37347383 -0.75870339]\r\n",
    " [ 0.6119356  -1.62743362]]\r\n",
    "b[1] =\r\n",
    "[[0.]\r\n",
    " [0.]\r\n",
    " [0.]]\r\n",
    "W[2] =\r\n",
    "[[ 1.00736754 -0.43948301  0.18419731]\r\n",
    " [-0.14397405  0.84414841 -1.18942279]\r\n",
    " [-0.18614766 -0.22173389  0.65458209]\r\n",
    " [-0.63502252 -0.09955147 -0.50683179]\r\n",
    " [ 0.02437212  0.33648852 -0.63544278]]\r\n",
    "b[2] =\r\n",
    "[[0.]\r\n",
    " [0.]\r\n",
    " [0.]\r\n",
    " [0.]\r\n",
    " [0.]]\r\n",
    "W[3] =\r\n",
    "[[ 0.51193601  0.40320363  0.2247223   0.40287503 -0.30577239]]\r\n",
    "b[3] =\r\n",
    "[[0.]]\r\n",
    "```\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def forward_propagation_step(A_prev, W, b, activation):\r\n",
    "    \"\"\"\r\n",
    "    A_prev - activations from previous layer: (size of previous layer, number of examples)\r\n",
    "    W - weights matrix: array of shape (size of current layer, size of previous layer)\r\n",
    "    b - bias vector, array of shape (size of the current layer, 1)\r\n",
    "    activation - text string \"sigmoid\" or \"relu\"\r\n",
    "\r\n",
    "    Returns:\r\n",
    "    A -  post-activation value \r\n",
    "    cache - tuple containing W, b, A_prev, Z stored for computing the backward pass\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    Z = None\r\n",
    "    \r\n",
    "    if activation == \"sigmoid\":\r\n",
    "        A = None\r\n",
    "        \r\n",
    "    elif activation == \"relu\":\r\n",
    "        A = None\r\n",
    "\r\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\r\n",
    "\r\n",
    "    cache = (W, b, A_prev, Z) # used at backward propagation. Note: b looks as need just to check the shape of dJ_db\r\n",
    "    return A, cache\r\n",
    "\r\n",
    "\r\n",
    "def sigmoid(Z):\r\n",
    "    return 1/(1+np.exp(-Z))\r\n",
    "\r\n",
    "def relu(Z):\r\n",
    "    return np.maximum(0,Z)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Compute cost\n",
    "\n",
    "</font>\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def compute_cost(A_last, Y):\r\n",
    "    \"\"\"\r\n",
    "    A_last - vector of predicted probabilties - activations of last layer L, shape (1, number of examples)\r\n",
    "    Y - true label e.g. cat vs non-cat, shape (1, number of examples)\r\n",
    "    Returns:\r\n",
    "    cost - cross-entropy cost\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    assert (A_last.shape == Y.shape)\r\n",
    "    cost = None\r\n",
    "\r\n",
    "    assert(cost.shape == ()) \r\n",
    "    \r\n",
    "    return cost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Init backward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def init_backward_propagation(Y, A_last):\r\n",
    "    dL_dA_last =  None\r\n",
    "    return dL_dA_last"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def backward_propagation_step(dL_dA, cache, activation):\r\n",
    "    \"\"\"\r\n",
    "    dL_dA - activation gradient for current layer l\r\n",
    "    cache - (W, b, A_prev, Z) stored for current layer  l\r\n",
    "    activation - string: \"sigmoid\" or \"relu\"\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "    dL_dA_prev - Gradient activation of the previous layer l-1, same shape as A_prev\r\n",
    "    dL_dW - Gradient of W current layer l, same shape as W\r\n",
    "    dL_db - Gradient of b (current layer l), same shape as b\r\n",
    "    \"\"\"\r\n",
    "    W, b, A_prev, Z = cache \r\n",
    "\r\n",
    "    # backward activation part:\r\n",
    "    if activation == \"relu\":\r\n",
    "        dg_dz = None\r\n",
    "    elif activation == \"sigmoid\":\r\n",
    "        dg_dz = None\r\n",
    "        \r\n",
    "    assert (dL_dA.shape == dg_dz.shape)\r\n",
    "    dL_dZ = None\r\n",
    "\r\n",
    "    # backward linear part:\r\n",
    "   \r\n",
    "    dL_dW = None\r\n",
    "    dL_db = None\r\n",
    "    dL_dA_prev = None\r\n",
    "    \r\n",
    "\r\n",
    "    assert (dL_dA_prev.shape == A_prev.shape)\r\n",
    "    assert (dL_dW.shape == W.shape)\r\n",
    "    assert (dL_db.shape == b.shape)\r\n",
    "\r\n",
    "    return dL_dA_prev, dL_dW, dL_db\r\n",
    "        \r\n",
    "\r\n",
    "def relu_backward(Z):\r\n",
    "    dg_dz = None\r\n",
    "    assert (dg_dz.shape == Z.shape)    \r\n",
    "    return dg_dz\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def sigmoid_backward(Z):\r\n",
    "\r\n",
    "    dg_dz = None\r\n",
    "    assert (dg_dz.shape == Z.shape)    \r\n",
    "    return dg_dz\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Update parameters \n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\r\n",
    "    \"\"\"\r\n",
    "    Update parameters due to gradient descent rule \r\n",
    "    parameters - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \r\n",
    "    grads - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \r\n",
    "   \r\n",
    "    Returns: updated parameters the same shape as input parameters \r\n",
    "    \"\"\"\r\n",
    "   \r\n",
    "    parameters = None\r\n",
    "    return parameters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build two-layer model\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    '''\n",
    "    X - input layer of shape (input size, number of examples)\n",
    "    Y - output layer of shape (1,m)\n",
    "    layers_dims - list of layers dims including input layer \n",
    "    '''\n",
    "  \n",
    "    np.random.seed(1)\n",
    "    grads = {'W':{}, 'b':{}}\n",
    "    costs = []   # track the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "\n",
    "    # Initialize parameters \n",
    "    parameters = None\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(None):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        A1, cache1 = None\n",
    "        A2, cache2 = None\n",
    "\n",
    "        # Compute cost        \n",
    "        cost = None\n",
    "                \n",
    "        # Initialize backward propagation        \n",
    "        dL_dA2 = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        dL_dA1, grads['W'][2], grads['b'][2] = None\n",
    "        _, grads['W'][1], grads['b'][1] = None\n",
    "        \n",
    "       \n",
    "        # Update parameters    \n",
    "        parameters =  None\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    None\n",
    "\n",
    "    \n",
    "    return parameters\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Train the model learning\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = two_layer_model(\n",
    "    X_train_scaled, Y_train, layers_dims, learning_rate = 0.003, num_iterations = 3000, print_cost=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Cost after iteration 0: 0.6950464961800915\n",
    "Cost after iteration 100: 0.6195808854384666\n",
    "Cost after iteration 200: 0.5865026104533535\n",
    "Cost after iteration 300: 0.5467810398248231\n",
    "Cost after iteration 400: 0.49825722524914073\n",
    "Cost after iteration 500: 0.4565738532427634\n",
    "Cost after iteration 600: 0.4094471539583378\n",
    "Cost after iteration 700: 0.3631730375845946\n",
    "Cost after iteration 800: 0.32861131098831003\n",
    "Cost after iteration 900: 0.29718068617894683\n",
    "Cost after iteration 1000: 0.269008932771751\n",
    "Cost after iteration 1100: 0.24464399315834634\n",
    "Cost after iteration 1200: 0.22384078814076205\n",
    "Cost after iteration 1300: 0.20652180532903278\n",
    "Cost after iteration 1400: 0.1893838186161027\n",
    "Cost after iteration 1500: 0.17501499357058234\n",
    "Cost after iteration 1600: 0.16159871226192452\n",
    "Cost after iteration 1700: 0.14745343426258842\n",
    "Cost after iteration 1800: 0.13478215612014718\n",
    "Cost after iteration 1900: 0.1231337962128783\n",
    "Cost after iteration 2000: 0.11228120642500024\n",
    "Cost after iteration 2100: 0.10296514643048342\n",
    "Cost after iteration 2200: 0.09462444340407362\n",
    "Cost after iteration 2300: 0.08577949788493319\n",
    "Cost after iteration 2400: 0.07838067328265923\n",
    "Cost after iteration 2500: 0.07228557169893012\n",
    "Cost after iteration 2600: 0.06685754016383366\n",
    "Cost after iteration 2700: 0.06210220111202463\n",
    "Cost after iteration 2800: 0.05787619296178021\n",
    "Cost after iteration 2900: 0.053884527381680314`\n",
    "\n",
    "<img src = \"data/19_check_2_layer.png\">\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Evaluate 2-layers model\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def evaluate_two_layers(X, Y, parameters):\n",
    "    \"\"\"        \n",
    "    X - array to predict \n",
    "    parameters - parameters of the trained model\n",
    "    \"\"\"\n",
    "  \n",
    "    # Forward propagation\n",
    "    A1,_ = None\n",
    "    Y_pred, _ = None\n",
    "\n",
    "    accuracy =  None\n",
    "    print(\"Accuracy: {:.3f}\".format( accuracy))\n",
    "       \n",
    "\n",
    "evaluate_two_layers(X_train_scaled, Y_train, parameters)\n",
    "test_accuracy = evaluate_two_layers(X_test_scaled, Y_test, parameters)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation whole process\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "\n",
    "def forward_propagation_whole_process(X, parameters):\n",
    "    \"\"\"\n",
    "    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    X - data, array of shape (input size, number of examples)\n",
    "    parameters - initialized parameters foreach of 'W' and 'b' keas values have keys 1,2,...L \n",
    "    \n",
    "    Returns:\n",
    "    A_last - last activation value (y_pred)\n",
    "    caches - dict of caches containing every cache of forward propagation indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters['W']) # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = None\n",
    "        A, cache = None\n",
    "        caches[l] = None\n",
    "\n",
    "    \n",
    "    #LINEAR -> SIGMOID\n",
    "    A_last, cache = None\n",
    "    caches[L] = None\n",
    "\n",
    "    assert(A_last.shape == (1, X.shape[1])) # (1,m) \n",
    "            \n",
    "    return A_last, caches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation whole process\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def backward_propagation_whole_process(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    A_last - probability vector, output(y_pred) of the forward propagation \n",
    "    Y - true labels (0 if non-cat, 1 if cat)\n",
    "    caches - dict of caches for each layer that contains (W, b, A, Z)\n",
    "    Returns: grads - of keys 'W' and 'b' each containing the  dictionaries of keys 1..L  \n",
    "    \"\"\"\n",
    "    dL_dA= {}\n",
    "    dL_dW = {}\n",
    "    dL_db= {}\n",
    "    \n",
    "    L = len(caches) # the number of layers\n",
    "    m = None # number of samples\n",
    "    Y = Y.reshape(A_last.shape) # make sure Y is the same shape as A_last(y_pred)\n",
    "    \n",
    "    # Initialize the backpropagation    \n",
    "    dL_dA[L] = None\n",
    "\n",
    "    # layer (SIGMOID -> LINEAR) gradients\n",
    "    current_cache = None\n",
    "    dL_dA[L-1], dL_dW[L], dL_db[L] = None\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(1,L)): #  starts with L-1 ends with 1 \n",
    "        # l-th layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = None\n",
    "        dL_dA[l-1], dL_dW[l], dL_db[l] = None\n",
    "        \n",
    "    grads= None\n",
    "    \n",
    "    return grads\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build deep neural network model \n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, verbose = False):\n",
    "    \"\"\"\n",
    "    X - data, array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y - true label vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims - list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate - learning rate of the gradient descent update rule\n",
    "    num_iterations - number of iterations of the optimization loop\n",
    "    verbose - if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters - parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    print ('Training {}-layers neural network with layers dimensions: {}'.format (len(layers_dims)-1, layers_dims))\n",
    "    np.random.seed(1)\n",
    "    costs = [] # to track of cost\n",
    "            \n",
    "    parameters = None\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        A_last, caches = None\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = None\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = None\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    None\n",
    "    \n",
    "   \n",
    "    return parameters\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 2 layers \n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 5000, verbose = True, \n",
    "    learning_rate = 0.003) \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Training 2-layers neural network with layers dimensions: [12288, 7, 1]\n",
    "Cost after iteration 0: 0.6950464961800915\n",
    "Cost after iteration 100: 0.6195808854384666\n",
    "Cost after iteration 200: 0.5865026104533535\n",
    "Cost after iteration 300: 0.5467810398248231\n",
    "Cost after iteration 400: 0.49825722524914073\n",
    "...\n",
    "Cost after iteration 4500: 0.023333388131945094\n",
    "Cost after iteration 4600: 0.022437269637948253\n",
    "Cost after iteration 4700: 0.021604560218738137\n",
    "Cost after iteration 4800: 0.02083238901344186\n",
    "Cost after iteration 4900: 0.020094155779688965`\n",
    "\n",
    "<img src = \"data/19_check_2_layer_2.png\">\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 4 layers \n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "n_x = X_train_flatten.shape[0]\n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x,20,7,5,n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 3000, verbose = True, \n",
    "    learning_rate = 0.0075) \n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Training 4-layers neural network with layers dimensions: [12288, 20, 7, 5, 1]\n",
    "Cost after iteration 0: 0.7717493284237688\n",
    "Cost after iteration 100: 0.6720534400822913\n",
    "Cost after iteration 200: 0.6482632048575212\n",
    "Cost after iteration 300: 0.6115068816101354\n",
    "Cost after iteration 400: 0.567047326836611\n",
    "...\n",
    "Cost after iteration 2500: 0.08841251177615041\n",
    "Cost after iteration 2600: 0.08595130416146428\n",
    "Cost after iteration 2700: 0.08168126914926334\n",
    "Cost after iteration 2800: 0.07824661275815534\n",
    "Cost after iteration 2900: 0.07544408693855482`\n",
    "\n",
    "<img src = \"data/19_check_4_layer.png\">\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Predict \n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"        \n",
    "    X - array set to predict \n",
    "    parameters - parameters of the trained model\n",
    "    Returns:\n",
    "    Y_pred - predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward propagation\n",
    "    A_last, _ = None\n",
    "    Y_pred= None\n",
    "    \n",
    "    return Y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Evaluate\n",
    "\n",
    "</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "\n",
    "Y_pred_train = predict (X_train_scaled, parameters)\n",
    "Y_pred_test = predict (X_test_scaled, parameters)\n",
    "\n",
    "print ('Train accuracy = {:.3%}'.format(np.mean (Y_pred_train == Y_train)))\n",
    "print ('Test accuracy = {:.3%}'.format(np.mean (Y_pred_test == Y_test)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Train accuracy = 99.043%\n",
    "Test accuracy = 82.000%`\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "\n",
    "</font>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "3db8efa666cb8a96b7ce6ded4fbc1cca6ce4d9125d0af5a3a7517411ce1d0385"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}