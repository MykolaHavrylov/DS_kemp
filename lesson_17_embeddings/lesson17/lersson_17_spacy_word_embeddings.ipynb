{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "If you’re working with a lot of text, you’ll eventually want to know more about it. For example, what’s it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Features\n",
    "\n",
    "| NAME                              | DESCRIPTION                                                                                                        |\n",
    "|-----------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| Tokenization                      | Segmenting text into words, punctuations marks etc.                                                                |\n",
    "| Part-of-speech (POS) Tagging      | Assigning word types to tokens, like verb or noun.                                                                 |\n",
    "| Dependency Parsing                | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object. |\n",
    "| Lemmatization                     | Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.      |\n",
    "| Sentence Boundary Detection (SBD) | Finding and segmenting individual sentences.                                                                       |\n",
    "| Named Entity Recognition (NER)    | Labelling named “real-world” objects, like persons, companies or locations.                                        |\n",
    "| Entity Linking (EL)               | Disambiguating textual entities to unique identifiers in a knowledge base.                                         |\n",
    "| Similarity                        | Comparing words, text spans and documents and how similar they are to each other.                                  |\n",
    "| Text Classification               | Assigning categories or labels to a whole document, or parts of a document.                                        |\n",
    "| Rule-based Matching               | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.       |\n",
    "| Training                          | Updating and improving a statistical model’s predictions.                                                          |\n",
    "| Serialization                     | Saving objects to files or byte strings.                                                                           |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downloading models\n",
    "\n",
    "https://spacy.io/usage/models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !python -m spacy download en_core_web_lg\r\n",
    "# or you can use this solution from StackOverflow - https://stackoverflow.com/questions/55742788/ssl-certificate-verify-failed-error-while-downloading-python-m-spacy-download"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating nlp pipeline\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\". You can use the nlp object like a function to analyze text.It contains all the different components in the pipeline.\n",
    "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages that are available in spacy.lang"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import en_core_web_sm as en_core\r\n",
    "nlp = en_core.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp.pipe_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Doc object\n",
    "\n",
    "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
    "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Created by processing a string of text with the nlp object\r\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\r\n",
    "\r\n",
    "# Iterate over tokens in a Doc\r\n",
    "for token in doc:\r\n",
    "    print(token.text)\r\n",
    "    \r\n",
    "print(type(doc))\r\n",
    "doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Token object\n",
    "\n",
    "Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
    "To get a token at a specific position, you can index into the doc.\n",
    "Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text.\n",
    "\n",
    "<img src=\"./img/spacy1.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\r\n",
    "\r\n",
    "# Index into the Doc to get a single Token\r\n",
    "token = doc[2]\r\n",
    "\r\n",
    "# Get the token text via the .text attribute\r\n",
    "print(token.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "Segment text into words, punctuations marks, etc.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = nlp.tokenizer(\"\"\"\r\n",
    "Tokenization segments text into words, punctuations marks, etc. \r\n",
    "It is smarter than regex. \r\n",
    "It won't split the U.K. for example.\r\n",
    "\"\"\")\r\n",
    "list(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### POS tagging\n",
    "\n",
    "\n",
    "For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"She ate the pizza. Billion is a number\")\r\n",
    "\r\n",
    "# Iterate over the tokens\r\n",
    "for token in doc:\r\n",
    "    # Print the text and the predicted part-of-speech tag\r\n",
    "    print(token.text, token.pos_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting Syntactic Dependencies\n",
    "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
    "\n",
    "The .dep_ attribute returns the predicted dependency label.\n",
    "\n",
    "The .head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "\n",
    "<img src=\"img/1.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"She ate the pizza\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for 1 billion of dollars\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Named Entity Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\r\n",
    "\r\n",
    "# Iterate over the predicted entities\r\n",
    "for ent in doc.ents:\r\n",
    "    # Print the entity text and its label\r\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hint: Spacy Explain\r\n",
    "To get definitions for the most common tags and labels, you can use the spacy.explain helper function.\r\n",
    "\r\n",
    "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy.explain can tell you that it refers to countries, cities and states.\r\n",
    "\r\n",
    "The same works for part-of-speech tags and dependency labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(spacy.explain(\"GPE\"))\r\n",
    "print(spacy.explain(\"nsubj\"))\r\n",
    "print(spacy.explain(\"DET\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.\n",
    "\n",
    "The Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "\n",
    "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc1 = nlp(\"I like fast food\")\r\n",
    "doc2 = nlp(\"I like burgers\")\r\n",
    "doc3 = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\r\n",
    "print(doc1.similarity(doc2))\r\n",
    "print(doc1.similarity(doc3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://spacy.io/usage/spacy-101"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word representation\n",
    "\n",
    "Recall onehot  representation\n",
    "\n",
    "$$boy \\qquad \\qquad \\, girl\\quad \\qquad \\quad apple\\qquad \\quad orange\\qquad \\qquad king\\qquad \\qquad queen\\qquad \\qquad\\\\ \\begin{bmatrix} 0 \\\\ \\vdots  \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\quad  \\\\ \\vdots  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ 0 \\end{bmatrix}\\begin{matrix} \\quad  \\\\ \\leftarrow 1458 \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\end{matrix}\\begin{bmatrix} 0 \\\\ \\vdots  \\\\ \\quad  \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ 0 \\end{bmatrix}\\begin{matrix} \\quad  \\\\ \\quad  \\\\ \\leftarrow 3945 \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\end{matrix}\\begin{bmatrix} 0 \\\\ \\vdots  \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ 0 \\end{bmatrix}\\begin{matrix} \\leftarrow 472 \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\end{matrix}\\begin{bmatrix} 0 \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\vdots  \\\\ \\quad  \\\\ \\quad  \\\\ 1 \\\\ 0 \\\\ \\vdots  \\\\ 0 \\end{bmatrix}\\begin{matrix} \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\leftarrow 6117 \\\\ \\quad  \\end{matrix}\\begin{bmatrix} 0 \\\\ \\quad  \\\\ \\vdots  \\\\ \\quad  \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\quad  \\\\ \\vdots  \\\\ \\quad  \\\\ 0 \\end{bmatrix}\\begin{matrix} \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\leftarrow 4924 \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\end{matrix}\\begin{bmatrix} 0 \\\\  \\\\ \\quad  \\\\ \\quad  \\\\ \\vdots  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\\begin{matrix} \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\quad  \\\\ \\leftarrow 9714\\quad  \\\\ \\quad  \\end{matrix}\\\\ \\quad$$\n",
    "\n",
    "\n",
    "This representation does not provide any relation for similar words e.g. \n",
    "<br>`I like apple juice` and   `I like orange juice` \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target is to get **vectorized reprezentation** even if vectors are not semantically defined. \n",
    "\n",
    "The target vectors are expected to be similar by cosine similairity for semantically similar words. \n",
    "\n",
    "<img src=\"img/2.png\" align = 'left' style=\"width:350;height:250px;\"> <br>\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "If model is trained using such vectors then having in traing set $\\quad$ `I like apple` <u><b> juice </b></u>,  <br>  would make easier to predict $\\quad$ <u><b> juice </b></u> $\\quad$ for $\\quad$  `I like orange` <u><b> $\\text{_____}$ </b></u>\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"I have a banana\")\r\n",
    "# Access the vector via the token.vector attribute\r\n",
    "print(len(doc[3].vector))\r\n",
    "print(doc[3].vector)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec\n",
    "https://uk.wikipedia.org/wiki/Word2vec\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying show me your friends, and I'll tell who you are. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words shocked,appalled and astonished are typically used in a similar context.\n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the original Word2Vec implementation by Google and extended with additional functionality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training word2vec model using gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gzip\r\n",
    "import gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_file = \"./data/reviews_data.txt.gz\"\r\n",
    "\r\n",
    "with gzip.open(input_file, 'rb') as f:\r\n",
    "    for i,line in enumerate (f):\r\n",
    "        print(line)\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_input(input_file):\r\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\r\n",
    "    \r\n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\r\n",
    "    \r\n",
    "    with gzip.open(input_file, 'rb') as f:\r\n",
    "        for i, line in enumerate (f): \r\n",
    "            if (i%10000==0):\r\n",
    "                print(\"read {0} reviews\".format(i))\r\n",
    "            # do some pre-processing and return a list of words for each review text\r\n",
    "            yield gensim.utils.simple_preprocess(line)\r\n",
    "\r\n",
    "# read the tokenized reviews into a list\r\n",
    "# each review item becomes a serries of words\r\n",
    "# so this becomes a list of lists\r\n",
    "documents = list(read_input(input_file))\r\n",
    "print(\"Done reading data file\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the documents). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary.\r\n",
    "\r\n",
    "After building the vocabulary, we just need to call train(...) to start training the Word2Vec model. Training on the OpinRank dataset takes about 10 minutes so please be patient while running your code on this dataset.\r\n",
    "\r\n",
    "<img src=\"img/7.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = gensim.models.Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=24)\r\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "# with open(\"./w2v.model\", \"wb\") as f:\r\n",
    "#     pickle.dump(model, f)\r\n",
    "    \r\n",
    "with open(\"./w2v.model\", \"rb\") as f:\r\n",
    "    model = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_v= model.wv.get_vector('hello')\r\n",
    "print(len(word_v))\r\n",
    "print(word_v)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now, let's look at some output\n",
    "This first example shows a simple case of looking up words similar to the word dirty. All we need to do here is to call the most_similar function and provide the word dirty as the positive example. This returns the top 10 similar words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w1 = \"dirty\"\r\n",
    "model.wv.most_similar(positive=w1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w1 = [\"polite\"]\r\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w1 = [\"vehicle\"]\r\n",
    "model.wv.most_similar(positive=w1,topn=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that relate to bed only:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Analogy reasoning\n",
    "\n",
    "$boy - girl =  \\begin{bmatrix} \\text{~ 0} \\\\ \\text{~ 2} \\\\ \\text{~ 0} \\\\ \\text{~ 0} \\\\  \\text{~ 0} \\\\ \\vdots \\\\ \\text{~ 0}  \\\\ \\end{bmatrix} \\qquad king - queen =  \\begin{bmatrix} \\text{~ 0} \\\\ \\text{~ 2} \\\\ \\text{~ 0} \\\\ \\text{~ 0} \\\\  \\text{~ 0} \\\\ \\vdots \\\\ \\text{~ 0}  \\\\ \\end{bmatrix}$\n",
    "\n",
    "We may say:  `boy` - `girl` = `king` - `queen`\n",
    "\n",
    "Furher, answer the questions like `boy` is to `girl` as `king` is to `WHO?`\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\r\n",
    "\r\n",
    "# king - man + woman = ?\r\n",
    "\r\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = model.wv.most_similar(positive=[\"doctor\", \"woman\"], negative=['man'], topn=1)\r\n",
    "\r\n",
    "# doctor - man + woman = ?\r\n",
    "\r\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = model.wv.most_similar(positive=[\"huge\", \"small\"], negative=['big'], topn=1)\r\n",
    "\r\n",
    "# huge - big + small = ?\r\n",
    "\r\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Similarity between two words in the vocabulary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('similarity between two identical words')\r\n",
    "print(model.wv.similarity(w1=\"dirty\",w2=\"dirty\"))\r\n",
    "\r\n",
    "print('\\nsimilarity between two different words')\r\n",
    "print(model.wv.similarity(w1=\"dirty\",w2=\"smelly\"))\r\n",
    "print(model.wv.similarity(w1=\"bye\",w2=\"goodbye\"))\r\n",
    "print(model.wv.similarity(w1=\"car\",w2=\"vehicle\"))\r\n",
    "\r\n",
    "print('\\nsimilarity between two opposit words')\r\n",
    "print(model.wv.similarity(w1=\"dirty\",w2=\"clean\"))\r\n",
    "print(model.wv.similarity(w1=\"wet\",w2=\"dry\"))\r\n",
    "\r\n",
    "print('\\nsimilarity between two unrelated words')\r\n",
    "print(model.wv.similarity(w1=\"green\",w2=\"hotel\"))\r\n",
    "print(model.wv.similarity(w1=\"hello\",w2=\"the\"))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under the hood, it computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that dirty is highly similar to smelly but dirty is dissimilar to clean. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Which one is the odd one out in this list?\r\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Which one is the odd one out in this list?\r\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"sheet\",\"shower\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### USE - Universal Sentence Encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n",
    "\n",
    "The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import tensorflow_hub as hub\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\r\n",
    "# It may take a while\r\n",
    "\r\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \r\n",
    "# embed = hub.Module(module_url)\r\n",
    "\r\n",
    "# or download the model and use it locally\r\n",
    "\r\n",
    "embed = tf.saved_model.load(\"./USE/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\r\n",
    "word = \"Elephant\"\r\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n",
    "paragraph = (\r\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n",
    "    \"the more 'diluted' the embedding will be.\")\r\n",
    "messages = [word, sentence, paragraph]\r\n",
    "\r\n",
    "\r\n",
    "message_embeddings = embed(messages)\r\n",
    "\r\n",
    "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\r\n",
    "    print(\"Message: {}\".format(messages[i]))\r\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\r\n",
    "    message_embedding_snippet = \", \".join(\r\n",
    "        (str(x) for x in message_embedding[:3]))\r\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "str1 = \"I like my bike\"\r\n",
    "str2 = \"My motorcycle looks good\"\r\n",
    "str3 = \"Two more cells and we are done\"\r\n",
    "\r\n",
    "messages = [str1, str2, str3]\r\n",
    "emb1, emb2, emb3 = embed(messages)\r\n",
    "\r\n",
    "print(f\"The similarity between '{str1}' and '{str2}' = {np.inner(emb1, emb2)}\")\r\n",
    "print(f\"The similarity between '{str1}' and '{str3}' = {np.inner(emb1, emb3)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_similarity(labels, features, rotation):\r\n",
    "    print(len(features))\r\n",
    "    corr = np.inner(features, features)\r\n",
    "    sns.set(font_scale=1.2)\r\n",
    "    g = sns.heatmap(\r\n",
    "      corr,\r\n",
    "      xticklabels=labels,\r\n",
    "      yticklabels=labels,\r\n",
    "      vmin=0,\r\n",
    "      vmax=1,\r\n",
    "      cmap=\"YlOrRd\")\r\n",
    "    g.set_xticklabels(labels, rotation=rotation)\r\n",
    "    g.set_title(\"Semantic Textual Similarity\")\r\n",
    "\r\n",
    "\r\n",
    "def run_and_plot(messages, encoding_tensor):\r\n",
    "    message_embeddings = encoding_tensor(messages)\r\n",
    "    plot_similarity(messages, message_embeddings, 90)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "messages = [\r\n",
    "    # Smartphones\r\n",
    "    \"I like my phone\",\r\n",
    "    \"My phone is not good.\",\r\n",
    "    \"Your cellphone looks great.\",\r\n",
    "\r\n",
    "    # Weather\r\n",
    "    \"Will it snow tomorrow?\",\r\n",
    "    \"Recently a lot of hurricanes have hit the US\",\r\n",
    "    \"Global warming is real\",\r\n",
    "\r\n",
    "    # Food and health\r\n",
    "    \"An apple a day, keeps the doctors away\",\r\n",
    "    \"Eating strawberries is healthy\",\r\n",
    "    \"Is paleo better than keto?\",\r\n",
    "\r\n",
    "    # Asking about age\r\n",
    "    \"How old are you?\",\r\n",
    "    \"what is your age?\",\r\n",
    "]\r\n",
    "\r\n",
    "run_and_plot(messages, embed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Home task"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Using a Spacy, create a keywords extractor that should do the following things:\r\n",
    " - Take some text (article like) as an input.\r\n",
    " - Remove all stop words from the text.\r\n",
    " - Extract all the Nouns from text and sort them by count and return in descending order with amount of occurrences. \r\n",
    " - Extract all the Verbs from text and sort them by count and return in descending order with amount of occurrences.  \r\n",
    " - Extract all the Numbers from text and sort them by count and return in descending order with amount of occurrences. \r\n",
    " - Extract all the Named Entities from the text, group them into 4 groups (Location, Person, Organization, Misc.) and return groups in descending order with amount of occurrences. \r\n",
    "\r\n",
    "\r\n",
    "2. Using multilingual USE, align strings in English and Russian texts:\r\n",
    " - Download multilingual USE model - https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\r\n",
    " - Read \"./data/corpora/en.txt\" and \"./data/corpora/ru.txt\" files\r\n",
    " - Align English strings with their Russian analogues using mUSE\r\n",
    " \r\n",
    " \r\n",
    "3. Using the USE, create a Duplicate Phrase Finder that will do the following:\r\n",
    " - Take some large text as an input.\r\n",
    " - Separates text to SENTENCES (phrases). \r\n",
    " - Finds semantically similar strings (cosine similarity >=0.80)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords, gutenberg\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import en_core_web_sm as en_core\r\n",
    "from typing import Tuple\r\n",
    "from nltk import FreqDist\r\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Using a Spacy, create a keywords extractor that should do the following things:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define nlp\r\n",
    "nlp = en_core.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take some text (article like) as an input."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = gutenberg.raw(\"milton-paradise.txt\")\r\n",
    "print(text[1:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_stopwords(words: list[str], stopwords_type: str) -> str:\r\n",
    "    \"\"\"Remove stop words from text\r\n",
    "\r\n",
    "    Args:\r\n",
    "        texts (list[str]): list of words\r\n",
    "        stopwords_type (str): stop words language\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        str: text with deleted stopwords\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # define stop words\r\n",
    "    stop_words = set(stopwords.words(stopwords_type)) \r\n",
    "\r\n",
    "    # creating a list that is not including stop words\r\n",
    "    filtered_words = [word for word in words if word not in stop_words]\r\n",
    "    \r\n",
    "    return \" \".join(filtered_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# preprocess\r\n",
    "text = text.replace(\"\\n\", \"\").split(\" \")\r\n",
    "\r\n",
    "# removing stopwords\r\n",
    "text = remove_stopwords(text, stopwords_type=\"english\")\r\n",
    "\r\n",
    "# result\r\n",
    "text[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use `nlp` to analyze words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use nlp\r\n",
    "analyzed_words = nlp(text)\r\n",
    "\r\n",
    "# result\r\n",
    "analyzed_words[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract tokens with specific language parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_lang_parts(words: spacy.tokens.doc.Doc, lang_part: str = None) -> list:\r\n",
    "    \"\"\"Take the decided language part from text or all tokens\r\n",
    "\r\n",
    "    Args:\r\n",
    "        words (spacy.tokens.doc.Doc): all words\r\n",
    "        lang_part (str, optional): language part that we want to extract. Defaults to None.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: result list with words or tokens\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # check if [lang_part] is not None\r\n",
    "    if lang_part:\r\n",
    "        # return list with words\r\n",
    "        return [\r\n",
    "            token.text \r\n",
    "            for token in words\r\n",
    "            if token.pos_ == lang_part]\r\n",
    "    else:\r\n",
    "        # return list with tokens\r\n",
    "        return [\r\n",
    "            token\r\n",
    "            for token in words\r\n",
    "        ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract NOUN(sort by desc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take all NOUNs\r\n",
    "NOUN_words = extract_lang_parts(analyzed_words, lang_part=\"NOUN\")\r\n",
    "\r\n",
    "# count NOUNs\r\n",
    "NOUN_count = FreqDist(NOUN_words)\r\n",
    "\r\n",
    "# check the result\r\n",
    "print(NOUN_count.most_common()[0:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract VERB(sort by desc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take all VERBs\r\n",
    "VERB_words = extract_lang_parts(analyzed_words, lang_part=\"VERB\")\r\n",
    "\r\n",
    "# count VERBs\r\n",
    "VERB_count = FreqDist(VERB_words)\r\n",
    "\r\n",
    "# check the result\r\n",
    "print(VERB_count.most_common()[0:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract NUM(sort by desc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take all NUMs\r\n",
    "NUM_words = extract_lang_parts(analyzed_words, lang_part=\"NUM\")\r\n",
    "\r\n",
    "# count NUMs\r\n",
    "NUM_count = FreqDist(NUM_words)\r\n",
    "\r\n",
    "# check the result\r\n",
    "print(NUM_count.most_common()[0:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract all the Named Entities from the text, group them into 4 groups (Location, Person, Organization, Misc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_entities(doc: spacy.tokens.doc.Doc, categories: list, with_label: bool = True) -> list:\r\n",
    "    \"\"\"Extract specific entities with values or no\r\n",
    "\r\n",
    "    Args:\r\n",
    "        doc (spacy.tokens.doc.Doc): tokens\r\n",
    "        categories (list): entities to extract\r\n",
    "        with_label (bool, optional): return with values(Country name, Person name etc.) or no. Defaults to True.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: result list\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # check if it is True or not\r\n",
    "    if with_label:\r\n",
    "        # return result list with values and labels\r\n",
    "        return [(token.text, token.label_) for token in doc.ents if token.label_ in categories]\r\n",
    "    else:\r\n",
    "        # return only labels\r\n",
    "        return [token.label_ for token in doc.ents if token.label_ in categories]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# first method, we have list[Tuple]\r\n",
    "ents = extract_entities(analyzed_words, categories=[\"LOC\", \"PERSON\", \"ORG\", \"GPE\"])\r\n",
    "\r\n",
    "# check result\r\n",
    "print(dict(Counter(elem[1] for elem in ents)))\r\n",
    "\r\n",
    "# second method, we have only labels\r\n",
    "ent_labels = extract_entities(analyzed_words, with_label=False, categories=[\"LOC\", \"PERSON\", \"ORG\", \"GPE\"])\r\n",
    "\r\n",
    "# check result\r\n",
    "print(FreqDist(ent_labels).most_common())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Using multilingual USE, align strings in English and Russian texts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load model\r\n",
    "embed = tf.saved_model.load(\"./USE/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read \"./data/corpora/en.txt\" and \"./data/corpora/ru.txt\" files\r\n",
    "\r\n",
    "en = []\r\n",
    "ru = []\r\n",
    "with open(\"./data/corpora/en.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
    "    for line in f.readlines()[:50]:\r\n",
    "        en.append(line.strip())\r\n",
    "        \r\n",
    "with open(\"./data/corpora/ru.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
    "    for line in f.readlines()[:50]:\r\n",
    "        ru.append(line.strip())\r\n",
    "\r\n",
    "en[0], ru[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Processing with data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embed_en = embed(en)\r\n",
    "embed_ru = embed(ru)\r\n",
    "\r\n",
    "len(embed_en), len(embed_ru)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Align English strings with their Russian analogues using USE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_similarity(labels: list, features: list):\r\n",
    "  corr = np.inner(features[0], features[1])\r\n",
    "  sns.set(font_scale=1)\r\n",
    "  g = sns.heatmap(\r\n",
    "      corr,\r\n",
    "      xticklabels=labels[0],\r\n",
    "      yticklabels=labels[1],\r\n",
    "      vmin=0,\r\n",
    "      vmax=1,\r\n",
    "      cmap=\"magma_r\"\r\n",
    "  )\r\n",
    "  g.set_title(\"Semantic Textual Similarity\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_similarity(labels=[en, ru], features=[embed_en, embed_ru])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Using the USE, create a Duplicate Phrase Finder that will do the following\r\n",
    "- Take some large text as an input.\r\n",
    "- Separates text to SENTENCES (phrases). \r\n",
    "- Finds semantically similar strings (cosine similarity >=0.80)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from nltk.corpus import gutenberg\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load model\r\n",
    "embed = tf.saved_model.load(\"./USE/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "source": [
    "# get file id\r\n",
    "gutenberg.fileids()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 471
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "source": [
    "# load text\r\n",
    "def load_text(file_id: str) -> str:\r\n",
    "    \"\"\"Load your text\r\n",
    "\r\n",
    "    Args:\r\n",
    "        file_id (str): file id you want to download from gutenberg\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        str: raw text\r\n",
    "    \"\"\"\r\n",
    "    return gutenberg.raw(file_id).replace(\"\\n\", \" \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "source": [
    "text_to_compare, text_to_compare_with = load_text(\"carroll-alice.txt\"), load_text(\"whitman-leaves.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "source": [
    "# tokenize text\r\n",
    "def tokenize_to_sent(text: str) -> list:\r\n",
    "    \"\"\"Tokenize your text to sentences\r\n",
    "\r\n",
    "    Args:\r\n",
    "        text (str): full text\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: sentence tokens\r\n",
    "    \"\"\"\r\n",
    "    return sent_tokenize(text)[:800]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "source": [
    "tokenized_to_comp, tokenized_to_comp_with = tokenize_to_sent(text_to_compare), tokenize_to_sent(text_to_compare_with)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "source": [
    "# define embeddings\r\n",
    "embeddings_to_comp = embed(tokenized_to_comp)\r\n",
    "embeddings_to_comp_with = embed(tokenized_to_comp_with)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "source": [
    "def find_similar_sents(embed_to_comp: np.array, embed_to_comp_with: np.array, texts_to_comp: list[str], texts_to_comp_with: list[str], similarity: float = 0.5) -> list[list]:\r\n",
    "    \"\"\"Finds semantically similar strings\r\n",
    "\r\n",
    "    Args:\r\n",
    "        embed_to_comp (np.array): array of scaled sentences to compare\r\n",
    "        embed_to_comp_with (np.array): array of scaled sentences to compare with\r\n",
    "        texts_to_comp (list[str]): list of sentences to compare\r\n",
    "        texts_to_comp_with (list[str]): list of senteces to compare with\r\n",
    "        similarity (float, optional): [description]. Defaults to 0.5.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list[list]: result list of similar strings\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # define empty list\r\n",
    "    results = []\r\n",
    "\r\n",
    "    # iterate by two lists with their sentences\r\n",
    "    for to_compare, to_sent in zip(embed_to_comp, texts_to_comp):\r\n",
    "        \r\n",
    "        for to_compare_with, to_sent_with in zip(embed_to_comp_with, texts_to_comp_with):\r\n",
    "\r\n",
    "            # check if sentences are not the same at all\r\n",
    "            if (np.array(to_compare) != np.array(to_compare_with)).all():\r\n",
    "\r\n",
    "                # calculate similarity\r\n",
    "                calced_similarity = np.inner(to_compare, to_compare_with)\r\n",
    "\r\n",
    "                # define data to add\r\n",
    "                to_add = {to_sent, to_sent_with, calced_similarity}\r\n",
    "\r\n",
    "                # check if calculated similarity is bigger the provided by user and if the current data is already exist in the list\r\n",
    "                if calced_similarity >= similarity and to_add not in results:\r\n",
    "\r\n",
    "                    # add to list\r\n",
    "                    results.append(to_add)\r\n",
    "\r\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check different texts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "source": [
    "find_similar_sents(embeddings_to_comp, embeddings_to_comp_with, tokenized_to_comp, tokenized_to_comp_with, similarity=.8)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{\"'Who are YOU?'\", 0.8118769, 'what are you?'}]"
      ]
     },
     "metadata": {},
     "execution_count": 488
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check the same text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "source": [
    "# load text\r\n",
    "loaded_text = load_text(\"melville-moby_dick.txt\")\r\n",
    "\r\n",
    "# tokenize\r\n",
    "tokenized_text = tokenize_to_sent(loaded_text)\r\n",
    "\r\n",
    "# use embed\r\n",
    "embeddings = embed(tokenized_text)\r\n",
    "\r\n",
    "# check text\r\n",
    "find_similar_sents(embeddings, embeddings, tokenized_text, tokenized_text, similarity=.85)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "472c026ac2569fdbd4a7bd0058fc52d81c15d27b6366e23ecde7104f403cf5a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}